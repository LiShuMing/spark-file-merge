
## **使用说明**

该工具是小文件合并工具，支持parquet、LZO和GZIP压缩的文件合并，使用SPARK进行计算，合并规则：

* 不合并已合并过的文件( merged__1506738853356__0.lzo这样的类型)；
* 小于100MB的文件；
* 根据文件后缀判断是否为parque、lzo、gzipt文件；

## **原理**

* 基于Spark的RDD操作，先读取所有的小文件，然后根据文件类型输出至合并文件；
* 每个操作目录在mysql保存其各状态位，并记录该次操作元信息，可以提供在某些状态下任务异常时的上下文信息；
* 逻辑比较简单，理论上可以保证数据不会丢失（如果存在丢失数据就是spark的锅了）；

## **使用：**
> 该小工具最终以zip包的形式，在mammut中通过提交任务组的方式提交该zip包，创建任务；

具体使用如下：
1. 通过提供的zip包创建任务；
2. 单击`hdfs_file_merge`节点，编辑任务信息；
3. 任务提供了默认的一些配置，用户可以根据自己实际情况进行变更（比如executor数量及内存等）；
4. 在执行参数栏，用户需要提供本次合并的任务根目录，具体使用如下；
5. 点击开始执行；

参数支持格式：
> input_dir [--enable-recursive-merge] [--skip-delete-source]

其中：
* input_dir：必选，输入需要合并的根目录；
* --skip-delete-source： 可选，不删除源文件；
* --enable-recursive-merge: 可选，可以递归地合并，如果不加该参数，发现目录下有其他目录时，会退出；

当用户选择跳过删除源文件时，最终的合并结果会放在一个临时路径中，其格式如下：
> `/tmp/${user}/parquet_mergetool/${input_dir}`

比如说合并`/user/mammut/hive_db/zxx_hivecreate_615.db/filemerge_lzo/prt=2017-9-29-1`路径下的小文件，会生成一个临时路径:
`hdfs://hz-cluster1/tmp/mammut/parquet_mergetool/user/mammut/hive_db/zxx_hivecreate_615.db/filemerge_lzo/prt=2017-9-29-1`

## **注意事项**
* 由于当前是基于文件后缀判断是否需要合并，在有些场景下生成的文件类型是没有后缀的（比如hive的overwrite操作等），暂时是不支持的；
* （注意）当前工具仍处在Release阶段，建议用户关键数据先不考虑使用，避免数据损坏情况；

## **问题**
1. 当上次任务有异常，可否重复提交？  
> 理论上是可以的，目前任务支持不同阶段终止后的重复提交，可以分为如下几种：

* 上次任务合并失败重复提交， 会根据当前目录下的小文件重新执行合并操作；
* 上次任务合并成功但删除文件失败后重复提交，会继续删除上次任务遗留小文件，然后退出；


